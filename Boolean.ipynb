{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25fe5514-c5a6-4b73-a332-964524295984",
   "metadata": {},
   "source": [
    "# Modelo Booleanno de Recuperación de la Información"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151f2ac4-b479-427c-bf3f-5eab0952f7d5",
   "metadata": {},
   "source": [
    "## Leer m documentos (corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0f674a-6333-4a05-80bf-2a5b715e33af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import fitz # open pdf\n",
    "import re #regex\n",
    "import unicodedata # use unicode\n",
    "\n",
    "class Document:\n",
    "    def _content(self):\n",
    "        raw_doc = self._read_raw_doc(self.doc_name)\n",
    "        content = self._clean_doc(raw_doc)\n",
    "        self.content = content\n",
    "\n",
    "    def _freq(self):\n",
    "        self.freq_table = self.freq_term_table(self.terms(self.content))\n",
    "        self.terms = self.terms_unique(self.content)\n",
    "\n",
    "    def __init__(self, path_to_doc):\n",
    "        self.doc_name = path_to_doc\n",
    "        self._content()\n",
    "        self._freq()\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.doc_name}\"\n",
    "        \n",
    "    def terms(self, clean_document):\n",
    "        return re.split(r'[^\\w]+', clean_document)\n",
    "\n",
    "    def terms_unique(self, clean_document):\n",
    "        splited_words = self.terms(clean_document)\n",
    "        terms = []\n",
    "        for w in splited_words:\n",
    "            if w not in terms:\n",
    "                terms.append(w)\n",
    "        return terms\n",
    "\n",
    "    def freq_term_table(self, terms):\n",
    "        freq_table = {}\n",
    "        for term in terms:\n",
    "            if term in freq_table:\n",
    "                freq_table[term] += 1\n",
    "            else:\n",
    "                freq_table[term] = 1\n",
    "        return freq_table\n",
    "\n",
    "    def _read_raw_doc(self, doc_name):\n",
    "        doc = fitz.open(doc_name)\n",
    "        text = []\n",
    "        for page in doc:\n",
    "            text.append(page.get_text())\n",
    "        return ''.join(text)\n",
    "\n",
    "    def _clean_doc(self, raw_doc):\n",
    "        # remove accents\n",
    "        ## decompose unicode glyphs\n",
    "        normalized_string = unicodedata.normalize('NFKD',  raw_doc)\n",
    "        ## if a glyhp is compose, use its base form\n",
    "        no_accent_string = ''.join([c for c in normalized_string if not unicodedata.combining(c)])\n",
    "        # remove punctuation marks\n",
    "        no_punctuation_string = re.sub(r'[^\\w]+', ' ', no_accent_string)\n",
    "        # strip text of document to only get the main content\n",
    "        return no_punctuation_string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2835d72-cdaf-48d7-b2f4-648aa8fd37e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = Document(\"documentos/1984.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46145f8d-9980-4509-b105-2c46bff1300a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(doc.content[:100])\n",
    "print(doc.doc_name)\n",
    "print(doc.terms)\n",
    "print(doc.freq_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dee4d9e-d6dc-4e12-8489-365967fa5a2b",
   "metadata": {},
   "source": [
    "## Generar diccionario de términos de todo el corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f9f354e5-6a6c-4b17-ada7-e0f172b8c503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ingresa la ruta a tu carpeta contenedora de archivos pdf:\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "documentos/1984.pdf\n",
      "documentos/facturar-datos.pdf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "dir_path = input(\"Ingresa la ruta a tu carpeta contenedora de archivos pdf:\\n\") or \"documentos/\"\n",
    "files = os.listdir(dir_path)\n",
    "corpus = []\n",
    "for file in files:\n",
    "    path_to_doc = os.path.join(dir_path, file)\n",
    "    print(path_to_doc)\n",
    "    corpus.append(Document(path_to_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e65c34-e3c6-48fc-a53e-fec0b5844e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in corpus:\n",
    "    print(d.content[:100])\n",
    "    print(d.doc_name)\n",
    "    print(d.terms)\n",
    "    print(d.freq_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9be735-7767-4d5a-9e2d-b805f4511893",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_terms = []\n",
    "for d in corpus:\n",
    "    corpus_terms += d.terms\n",
    "\n",
    "corpus_set = set(corpus_terms)\n",
    "print( corpus_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b39f60e-4292-4d1b-a688-182cce2ed6c2",
   "metadata": {},
   "source": [
    "## Aplicar eliminación de palabras vacías (stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ed2c64a2-a7cb-4d31-9db1-9270e4d3ac22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/erick-m/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9daed328-99d8-4604-8f4a-792e82d0cae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_no_stopwords = [ w for w in corpus_set if w not in stopwords.words('spanish') ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715d1448-e9de-4b1e-957e-755bdd9e88a3",
   "metadata": {},
   "source": [
    "## Aplicar una técnica de stemming para reducir el \"Tamaño\" de las palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "707c58bf-f5ad-4c3f-9b78-eafc60697561",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/erick-m/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package snowball_data to\n",
      "[nltk_data]     /Users/erick-m/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt') # tokenizer data\n",
    "nltk.download('snowball_data') # Snowball stemmer data\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "13ec2db5-76c3-4257-8c92-27a82e84d8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"spanish\")\n",
    "stems = [ stemmer.stem(t) for t in corpus_no_stopwords ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661151c1-a64b-4c73-b1c7-7663e6b8bec0",
   "metadata": {},
   "source": [
    "## Obtener una matriz binaria de la presencia de los términos en cada documento de todo el corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3e926288-96a4-43ab-9d25-64571566198d",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = []\n",
    "for i, d in enumerate(corpus):\n",
    "    doc_row = []\n",
    "    for t in corpus_no_stopwords:\n",
    "        if t in d.terms:\n",
    "            doc_row.append(True)\n",
    "        else:\n",
    "            doc_row.append(False)\n",
    "    matrix.append(doc_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efeecca-857e-4dde-b2ce-cd29a16de7b7",
   "metadata": {},
   "source": [
    "## Diseñar una tabla hash que permita obtener mediante su función. llave -> valor <=> stem -> documentos donde aparece stem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f95ab6-c1a4-4451-9095-179b0e67c6b8",
   "metadata": {},
   "source": [
    "## Leer la consulta booleana Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb2be11-54ee-450d-811c-a3caeee87732",
   "metadata": {},
   "source": [
    "### Aplicar stopword y stemming a la consulta Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b46fa58-e5dd-4e1a-8841-3db099235036",
   "metadata": {},
   "source": [
    "### Aplicar la notación postfijo para el procesamiento de recuperación de la consulta dada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28350fd3-e513-435b-80dd-fb4fbaf6923e",
   "metadata": {},
   "source": [
    "## Presentar los nombre de los documentos obtenidos por Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494d94e0-457a-4d3b-b541-9cd9dbac6e8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
