{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25fe5514-c5a6-4b73-a332-964524295984",
   "metadata": {},
   "source": [
    "# Modelo Booleanno de Recuperación de la Información"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151f2ac4-b479-427c-bf3f-5eab0952f7d5",
   "metadata": {},
   "source": [
    "## Leer m documentos (corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d74d170c-f1c3-496b-a4b5-84380c5e9753",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/erick-m/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/erick-m/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package snowball_data to\n",
      "[nltk_data]     /Users/erick-m/nltk_data...\n",
      "[nltk_data]   Package snowball_data is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "import fitz # open pdf\n",
    "import re #regex\n",
    "import unicodedata # use unicode\n",
    "nltk.download('punkt') # tokenizer data\n",
    "nltk.download('snowball_data') # Snowball stemmer data\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c0f674a-6333-4a05-80bf-2a5b715e33af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_stop_word(word):\n",
    "    return word in stopwords.words('spanish') or len(word) <= 3\n",
    "\n",
    "class Document:\n",
    "    def _content(self):\n",
    "        raw_doc = self._read_raw_doc(self.doc_name)\n",
    "        content = self._clean_doc(raw_doc)\n",
    "        self.content = content\n",
    "\n",
    "    def _freq(self):\n",
    "        self.freq_table = self.freq_term_table(self.terms(self.content))\n",
    "        self.terms = self.terms_unique(self.content)\n",
    "\n",
    "    def __init__(self, path_to_doc):\n",
    "        self.doc_name = path_to_doc\n",
    "        self._content()\n",
    "        self._freq()\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.doc_name}\"\n",
    "\n",
    "    def remove_stop_words(self):\n",
    "        self.clean_terms =  [ w for w in self.terms if not is_stop_word(w) ]\n",
    "        return self.clean_terms\n",
    "        \n",
    "    def stems(self):\n",
    "        stemmer = SnowballStemmer(\"spanish\")\n",
    "        self.stems = [stemmer.stem(t) for t in self.clean_terms]\n",
    "        return self.stems\n",
    "        \n",
    "    def terms(self, clean_document):\n",
    "        return re.split(r'[^\\w]+', clean_document)\n",
    "\n",
    "    def terms_unique(self, clean_document):\n",
    "        splited_words = self.terms(clean_document)\n",
    "        terms = []\n",
    "        for w in splited_words:\n",
    "            if w not in terms:\n",
    "                terms.append(w)\n",
    "        return terms\n",
    "\n",
    "    def freq_term_table(self, terms):\n",
    "        freq_table = {}\n",
    "        for term in terms:\n",
    "            if term in freq_table:\n",
    "                freq_table[term] += 1\n",
    "            else:\n",
    "                freq_table[term] = 1\n",
    "        return freq_table\n",
    "\n",
    "    def _read_raw_doc(self, doc_name):\n",
    "        doc = fitz.open(doc_name)\n",
    "        text = []\n",
    "        for page in doc:\n",
    "            text.append(page.get_text())\n",
    "        return ''.join(text)\n",
    "\n",
    "    def _clean_doc(self, raw_doc):\n",
    "        # remove accents\n",
    "        ## decompose unicode glyphs\n",
    "        normalized_string = unicodedata.normalize('NFKD',  raw_doc)\n",
    "        ## if a glyhp is compose, use its base form\n",
    "        no_accent_string = ''.join([c for c in normalized_string if not unicodedata.combining(c)])\n",
    "        # remove punctuation marks\n",
    "        no_punctuation_string = re.sub(r'[^\\w]+', ' ', no_accent_string)\n",
    "        # strip text of document to only get the main content\n",
    "        return no_punctuation_string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2835d72-cdaf-48d7-b2f4-648aa8fd37e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = Document(\"documentos/1984.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "46145f8d-9980-4509-b105-2c46bff1300a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1984 george orwell parte primera capitulo i era un dia luminoso y frio de abril y los relojes daban \n",
      "documentos/1984.pdf\n",
      "11325\n",
      "11325\n",
      "11007\n"
     ]
    }
   ],
   "source": [
    "print(doc.content[:100])\n",
    "print(doc.doc_name)\n",
    "print(len(doc.terms))\n",
    "print(len(doc.freq_table))\n",
    "doc.remove_stop_words()\n",
    "print(len(doc.stems))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dee4d9e-d6dc-4e12-8489-365967fa5a2b",
   "metadata": {},
   "source": [
    "## Generar diccionario de términos de todo el corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b55d18-3b65-408f-a69c-b0b5f68df70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "dir_path = input(\"Ingresa la ruta a tu carpeta contenedora de archivos pdf:\\n\") or \"documentos/\"\n",
    "file_paths = os.listdir(dir_path)\n",
    "complete_file_paths = [ os.path.join(dir_path, file) for file in file_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e681cf7-13e6-4777-bea9-3433f9448712",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "#import time\n",
    "# start = time.time()\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    corpus = executor.map(lambda path: Document(path), complete_file_paths)\n",
    "# print(f\"Pool : {time.time()-start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454e811b-669d-413e-a955-0fcf7be51938",
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_corpus = list(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f94dfb-4092-4c45-b447-f74faa22077c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( lista_corpus )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e65c34-e3c6-48fc-a53e-fec0b5844e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in lista_corpus:\n",
    "    print(d.doc_name)\n",
    "    print(d.terms[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9be735-7767-4d5a-9e2d-b805f4511893",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_terms = []\n",
    "for d in lista_corpus:\n",
    "    corpus_terms += d.terms\n",
    "\n",
    "# print( corpus_terms )\n",
    "# print( set(corpus_terms) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b39f60e-4292-4d1b-a688-182cce2ed6c2",
   "metadata": {},
   "source": [
    "## Aplicar eliminación de palabras vacías (stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2c64a2-a7cb-4d31-9db1-9270e4d3ac22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27089735-dc6b-4cf9-9b92-a66e14a5cc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_stop_word(word):\n",
    "    # if len(word) == 2:\n",
    "    #     print(f\"word: {word}\")\n",
    "    return word in stopwords.words('spanish') or len(word) <= 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9daed328-99d8-4604-8f4a-792e82d0cae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_no_stopwords = [ w for w in corpus_set if  not is_stop_word(w) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a37634e-1c95-422b-b220-6d6da0e55f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Aplicar una técnica de stemming para reducir el \"Tamaño\" de las palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707c58bf-f5ad-4c3f-9b78-eafc60697561",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt') # tokenizer data\n",
    "nltk.download('snowball_data') # Snowball stemmer data\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ec2db5-76c3-4257-8c92-27a82e84d8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"spanish\")\n",
    "stems = [ (stemmer.stem(t), t) for t in corpus_no_stopwords ]\n",
    "for s, t in stems:\n",
    "    if len(s) < 4:\n",
    "        if len(s) <=2:\n",
    "            print(f\"\\t\\t\\t{(s,t)}\")\n",
    "        else:\n",
    "            print(f\"{(s,t)}\", end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661151c1-a64b-4c73-b1c7-7663e6b8bec0",
   "metadata": {},
   "source": [
    "## Obtener una matriz binaria de la presencia de los términos en cada documento de todo el corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e926288-96a4-43ab-9d25-64571566198d",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = []\n",
    "for i, d in enumerate(corpus):\n",
    "    doc_row = []\n",
    "    for t in corpus_no_stopwords:\n",
    "        if t in d.terms:\n",
    "            doc_row.append(True)\n",
    "        else:\n",
    "            doc_row.append(False)\n",
    "    matrix.append(doc_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70696c8e-9fbb-47bc-b111-7ce5d3d382cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_stem = [stemmer.stem(w) for w in corpus[0].terms]\n",
    "print(len(doc_stem) == len(corpus[0].terms))\n",
    "print(len(corpus[0].terms))\n",
    "print(len(doc_stem))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efeecca-857e-4dde-b2ce-cd29a16de7b7",
   "metadata": {},
   "source": [
    "## Diseñar una tabla hash que permita obtener mediante su función. llave -> valor <=> stem -> documentos donde aparece stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dc1743-aca8-498a-bba1-18ef3532ebbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8f95ab6-c1a4-4451-9095-179b0e67c6b8",
   "metadata": {},
   "source": [
    "## Leer la consulta booleana Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb2be11-54ee-450d-811c-a3caeee87732",
   "metadata": {},
   "source": [
    "### Aplicar stopword y stemming a la consulta Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b46fa58-e5dd-4e1a-8841-3db099235036",
   "metadata": {},
   "source": [
    "### Aplicar la notación postfijo para el procesamiento de recuperación de la consulta dada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28350fd3-e513-435b-80dd-fb4fbaf6923e",
   "metadata": {},
   "source": [
    "## Presentar los nombre de los documentos obtenidos por Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494d94e0-457a-4d3b-b541-9cd9dbac6e8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
