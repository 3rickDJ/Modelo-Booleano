{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25fe5514-c5a6-4b73-a332-964524295984",
   "metadata": {},
   "source": [
    "# Modelo Booleanno de Recuperaci贸n de la Informaci贸n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151f2ac4-b479-427c-bf3f-5eab0952f7d5",
   "metadata": {},
   "source": [
    "## Leer m documentos (corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d74d170c-f1c3-496b-a4b5-84380c5e9753",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/erick-m/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/erick-m/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package snowball_data to\n",
      "[nltk_data]     /Users/erick-m/nltk_data...\n",
      "[nltk_data]   Package snowball_data is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "import fitz # open pdf\n",
    "import re #regex\n",
    "import unicodedata # use unicode\n",
    "nltk.download('punkt') # tokenizer data\n",
    "nltk.download('snowball_data') # Snowball stemmer data\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c0f674a-6333-4a05-80bf-2a5b715e33af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Document:\n",
    "    def _content(self):\n",
    "        raw_doc = self._read_raw_doc(self.doc_name)\n",
    "        content = self._clean_doc(raw_doc)\n",
    "        self.content = content\n",
    "\n",
    "    def _freq(self):\n",
    "        self.freq_table = self.freq_term_table(self.terms(self.content))\n",
    "        self.terms = self.terms_unique(self.content)\n",
    "\n",
    "    def __init__(self, path_to_doc):\n",
    "        self.doc_name = path_to_doc\n",
    "        self._content()\n",
    "        self._freq()\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.doc_name}\"\n",
    "\n",
    "    def remove_stop_words(self):\n",
    "        self.clean_terms =  [ w for w in self.terms if not self.is_stop_word(w) ]\n",
    "        return self.clean_terms\n",
    "        \n",
    "    def is_stop_word(self, word):\n",
    "        return word in stopwords.words('spanish') or len(word) <= 3\n",
    "        \n",
    "    def stemming(self):\n",
    "        self.remove_stop_words()\n",
    "        stemmer = SnowballStemmer(\"spanish\")\n",
    "        self.stems = [stemmer.stem(t) for t in self.clean_terms]\n",
    "        return self.stems\n",
    "        \n",
    "    def terms(self, clean_document):\n",
    "        return re.split(r'[^\\w]+', clean_document)\n",
    "\n",
    "    def terms_unique(self, clean_document):\n",
    "        splited_words = self.terms(clean_document)\n",
    "        terms = []\n",
    "        for w in splited_words:\n",
    "            if w not in terms:\n",
    "                terms.append(w)\n",
    "        return terms\n",
    "\n",
    "    def freq_term_table(self, terms):\n",
    "        freq_table = {}\n",
    "        for term in terms:\n",
    "            if term in freq_table:\n",
    "                freq_table[term] += 1\n",
    "            else:\n",
    "                freq_table[term] = 1\n",
    "        return freq_table\n",
    "\n",
    "    def _read_raw_doc(self, doc_name):\n",
    "        doc = fitz.open(doc_name)\n",
    "        text = []\n",
    "        for page in doc:\n",
    "            text.append(page.get_text())\n",
    "        return ''.join(text)\n",
    "\n",
    "    def _clean_doc(self, raw_doc):\n",
    "        # remove accents\n",
    "        ## decompose unicode glyphs\n",
    "        normalized_string = unicodedata.normalize('NFKD',  raw_doc)\n",
    "        ## if a glyhp is compose, use its base form\n",
    "        no_accent_string = ''.join([c for c in normalized_string if not unicodedata.combining(c)])\n",
    "        # remove punctuation marks\n",
    "        no_punctuation_string = re.sub(r'[^\\w]+', ' ', no_accent_string)\n",
    "        # strip text of document to only get the main content\n",
    "        return no_punctuation_string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2835d72-cdaf-48d7-b2f4-648aa8fd37e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = Document(\"documentos/1984.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46145f8d-9980-4509-b105-2c46bff1300a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1984 george orwell parte primera capitulo i era un dia luminoso y frio de abril y los relojes daban \n",
      "documentos/1984.pdf\n",
      "11325\n",
      "11325\n",
      "11007\n"
     ]
    }
   ],
   "source": [
    "print(doc.content[:100])\n",
    "print(doc.doc_name)\n",
    "print(len(doc.terms))\n",
    "print(len(doc.freq_table))\n",
    "doc.stemming()\n",
    "print(len(doc.stems))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dee4d9e-d6dc-4e12-8489-365967fa5a2b",
   "metadata": {},
   "source": [
    "## Generar diccionario de t茅rminos de todo el corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21b55d18-3b65-408f-a69c-b0b5f68df70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# dir_path = input(\"Ingresa la ruta a tu carpeta contenedora de archivos pdf:\\n\") or \"documentos/\"\n",
    "dir_path = \"documentos/\"\n",
    "file_paths = os.listdir(dir_path)\n",
    "complete_file_paths = [ os.path.join(dir_path, file) for file in file_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e681cf7-13e6-4777-bea9-3433f9448712",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "#import time\n",
    "# start = time.time()\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    corpus = executor.map(lambda path: Document(path), complete_file_paths)\n",
    "# print(f\"Pool : {time.time()-start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "454e811b-669d-413e-a955-0fcf7be51938",
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_corpus = list(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39f94dfb-4092-4c45-b447-f74faa22077c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<__main__.Document object at 0x10a667f90>, <__main__.Document object at 0x10a6684d0>]\n",
      "['documentos/1984.pdf', 'documentos/facturar-datos.pdf']\n"
     ]
    }
   ],
   "source": [
    "print( lista_corpus )\n",
    "docs_in_corpus = [ d.doc_name for d in lista_corpus]\n",
    "print(docs_in_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69e65c34-e3c6-48fc-a53e-fec0b5844e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "documentos/1984.pdf\n",
      "['1984', 'george', 'orwell', 'parte', 'primera', 'capitulo', 'i', 'era', 'un', 'dia']\n",
      "documentos/facturar-datos.pdf\n",
      "['datos', 'necesarios', 'para', 'facturar', 'se', 'tienen', 'que', 'enviar', 'en', 'el']\n"
     ]
    }
   ],
   "source": [
    "for d in lista_corpus:\n",
    "    print(d.doc_name)\n",
    "    print(d.terms[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db9be735-7767-4d5a-9e2d-b805f4511893",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_terms = []\n",
    "for d in lista_corpus:\n",
    "    corpus_terms += d.terms\n",
    "\n",
    "corpus_set = set(corpus_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16009d8e-f51e-482c-b45d-0dc65f1a2289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11488\n",
      "11394\n"
     ]
    }
   ],
   "source": [
    "print(len(corpus_terms))\n",
    "print(len(corpus_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b39f60e-4292-4d1b-a688-182cce2ed6c2",
   "metadata": {},
   "source": [
    "## Aplicar eliminaci贸n de palabras vac铆as (stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed2c64a2-a7cb-4d31-9db1-9270e4d3ac22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/erick-m/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27089735-dc6b-4cf9-9b92-a66e14a5cc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_stop_word(word):\n",
    "    return word in stopwords.words('spanish') or len(word) <= 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9daed328-99d8-4604-8f4a-792e82d0cae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_no_stopwords = [ w for w in corpus_set if  not is_stop_word(w) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1cb4ee34-ab59-4587-a8b1-524a2abe2988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11130\n",
      "11065\n"
     ]
    }
   ],
   "source": [
    "lista_corpus_noStopWords = []\n",
    "for d in lista_corpus:\n",
    "    d.remove_stop_words()\n",
    "    lista_corpus_noStopWords += d.clean_terms\n",
    "\n",
    "set_corpus_noStopWords = set(lista_corpus_noStopWords)\n",
    "print(len(lista_corpus_noStopWords))\n",
    "print(len(set_corpus_noStopWords))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab01ec0-de47-4b08-bbb5-8f5ae19e5a26",
   "metadata": {},
   "source": [
    "## Aplicar una t茅cnica de stemming para reducir el \"Tama帽o\" de las palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "707c58bf-f5ad-4c3f-9b78-eafc60697561",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/erick-m/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package snowball_data to\n",
      "[nltk_data]     /Users/erick-m/nltk_data...\n",
      "[nltk_data]   Package snowball_data is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt') # tokenizer data\n",
    "nltk.download('snowball_data') # Snowball stemmer data\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4f8c535-7a75-4313-9a1e-d8277554220f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"spanish\")\n",
    "stems = [ (stemmer.stem(t), t) for t in corpus_no_stopwords ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e103b801-03cf-4598-b77f-2d2406c6b9b3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for s, t in stems:\n",
    "#     if len(s) < 4:\n",
    "#         if len(s) <=2:\n",
    "#             print(f\"\\t\\t\\t{(s,t)}\")\n",
    "#         else:\n",
    "#             print(f\"{(s,t)}\", end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc8155a3-498d-4f0b-9208-ea277b7c27a3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for d in lista_corpus:\n",
    "    d.stemming()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b6e90f2-1f2f-4073-acee-3bf1965dd97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11130\n",
      "5534\n"
     ]
    }
   ],
   "source": [
    "stemmed_corpus = []\n",
    "for d in lista_corpus:\n",
    "    stemmed_corpus += d.stems\n",
    "set_stemmed_corpus = set(stemmed_corpus)\n",
    "print(len(stemmed_corpus))\n",
    "print(len(set_stemmed_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661151c1-a64b-4c73-b1c7-7663e6b8bec0",
   "metadata": {},
   "source": [
    "## Obtener una matriz binaria de la presencia de los t茅rminos en cada documento de todo el corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e926288-96a4-43ab-9d25-64571566198d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "matrix = [set_corpus_noStopWords]\n",
    "for d in lista_corpus:\n",
    "    doc_row = []\n",
    "    for t in set_corpus_noStopWords:\n",
    "        if t in d.terms:\n",
    "            doc_row.append(\"\")\n",
    "        else:\n",
    "            doc_row.append(\"\")\n",
    "    matrix.append(doc_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb7ae07b-365f-4a0b-b175-b0db4f1a6b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def write_csv(path, data):\n",
    "    with open(path, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        for row in data:\n",
    "            writer.writerow(row)\n",
    "write_csv('matrix.csv', matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af97ba30-8c7a-4824-94bf-71274edea12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_matrix = [set_stemmed_corpus]\n",
    "for i, d in enumerate(lista_corpus):\n",
    "    doc_row = []\n",
    "    for s in set_stemmed_corpus:\n",
    "        if s in d.stems:\n",
    "            doc_row.append(True)\n",
    "        else:\n",
    "            doc_row.append(False)\n",
    "    stem_matrix.append(doc_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6f026116-c8d9-49e5-af30-39fad2574d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_csv('stem_matrix.csv', stem_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efeecca-857e-4dde-b2ce-cd29a16de7b7",
   "metadata": {},
   "source": [
    "## Dise帽ar una tabla hash que permita obtener mediante su funci贸n. llave -> valor <=> stem -> documentos donde aparece stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "95dc1743-aca8-498a-bba1-18ef3532ebbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashtable\n",
    "tabla = hashtable.HashTable()\n",
    "\n",
    "tabla.insert(\"hola\", 2)\n",
    "def list_docs(stem):\n",
    "    docs = []\n",
    "    for d in lista_corpus:\n",
    "        if stem in d.stems:\n",
    "            docs.append(d.doc_name)\n",
    "    return docs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b01e2c3b-69ad-437c-b73e-0a785c9448e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in set_stemmed_corpus:\n",
    "    tabla.insert(s, list_docs(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f32e2aef-8694-47f0-b22f-5c2bc1024d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"tabla.hash\", \"wb\") as file:\n",
    "    pickle.dump(tabla, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "71f4401f-0cc6-4497-9b75-f45f1d1750cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['documentos/facturar-datos.pdf']\n",
      "['documentos/1984.pdf']\n",
      "['documentos/1984.pdf', 'documentos/facturar-datos.pdf']\n",
      "<function list_docs at 0x10a79b420>\n"
     ]
    }
   ],
   "source": [
    "print(tabla.find(\"factur\"))\n",
    "print(tabla.find(\"moj\"))\n",
    "print(tabla.find(\"archiv\"))\n",
    "print(list_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f95ab6-c1a4-4451-9095-179b0e67c6b8",
   "metadata": {},
   "source": [
    "## Leer la consulta booleana Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f30e3078-1bfe-469e-9356-f7275aca6f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restart\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package snowball_data to\n",
      "[nltk_data]     /Users/erick-m/nltk_data...\n",
      "[nltk_data]   Package snowball_data is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'query' from '/Users/erick-m/code/repos/RecuperacionInformacion/modelo_boleano/query.py'>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import query as query\n",
    "import importlib\n",
    "importlib.reload(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "122439e6-1589-45a3-8654-bdd4a11839a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_query = \"(FACTURAR u MOJAR) n (!(ARCHIVAR))\"\n",
    "raw_query2 = \"(FACTURAR u MOJAR) u (!(ARCHIVAR))\"\n",
    "stemmed_query = query.stemmed_query(raw_query)\n",
    "query.get_query(stemmed_query, tabla, docs_in_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "58331201-abac-4dcf-b80f-777f142d0a94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['documentos/facturar-datos.pdf', 'documentos/1984.pdf']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed_query2 = query.stemmed_query(raw_query2)\n",
    "query.get_query(stemmed_query2, tabla, docs_in_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb2be11-54ee-450d-811c-a3caeee87732",
   "metadata": {},
   "source": [
    "### Aplicar stopword y stemming a la consulta Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b46fa58-e5dd-4e1a-8841-3db099235036",
   "metadata": {},
   "source": [
    "### Aplicar la notaci贸n postfijo para el procesamiento de recuperaci贸n de la consulta dada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28350fd3-e513-435b-80dd-fb4fbaf6923e",
   "metadata": {},
   "source": [
    "## Presentar los nombre de los documentos obtenidos por Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494d94e0-457a-4d3b-b541-9cd9dbac6e8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
